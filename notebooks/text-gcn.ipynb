{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7CQrFOqntb_",
    "outputId": "fd7758b5-7bed-4038-bb49-0271971f5273"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSpe-DMYLL_H",
    "outputId": "6ee71205-68af-478c-9eec-d20e256baa78"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxJb9BCZCTYc",
    "outputId": "4f8d69a5-d449-48a8-bb80-9daed8732cd0"
   },
   "outputs": [],
   "source": [
    "!pip install flair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJPI-IXrBkrP"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dESRg5TlhmWI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkJf2jZkZmEO",
    "outputId": "b6c62013-134e-41c0-8494-563f9e1491f9"
   },
   "outputs": [],
   "source": [
    "df1_url=\"https://drive.google.com/file/d/1NpxtAbyvgjI6U0ss9BW8QUV7dAI45cTp/view?usp=drive_link\"\n",
    "df1_datapath='https://drive.google.com/uc?export=download&id='+df1_url.split('/')[-2]\n",
    "df1_train=pd.read_csv(df1_datapath)\n",
    "df1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utpsvnA6kaS-",
    "outputId": "201d2937-60c7-4ac9-8def-5181af76665a"
   },
   "outputs": [],
   "source": [
    "print(df1_train.dtypes)\n",
    "print(df1_train['Label'].isnull().sum())\n",
    "print(df1_train['Data'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gDrZIpHilHM",
    "outputId": "42d27fb6-4ee8-43d9-a507-df374ccff778"
   },
   "outputs": [],
   "source": [
    "df1_train.groupby(['Label']).size().plot.bar()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcsTMJj5i1iu",
    "outputId": "62eba8c6-16e1-4e44-e934-673332303609"
   },
   "outputs": [],
   "source": [
    "df1_train.groupby(['Label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTVU1tUcAgXh",
    "outputId": "ef53c48e-dd09-4579-89a2-98a5f338db4b"
   },
   "outputs": [],
   "source": [
    "df1_url=\"https://drive.google.com/file/d/12-0GHo9PEWpW-eXx0H5LYT0s8Y-TjhoX/view?usp=drive_link\"\n",
    "df1_datapath='https://drive.google.com/uc?export=download&id='+df1_url.split('/')[-2]\n",
    "df1_test=pd.read_csv(df1_datapath)\n",
    "df1_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AFZzdg1AgXk",
    "outputId": "2e22aed4-b919-45f1-bff4-24fd8965f0a2"
   },
   "outputs": [],
   "source": [
    "print(df1_test.dtypes)\n",
    "print(df1_test['Label'].isnull().sum())\n",
    "print(df1_test['Data'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-ZR0D13AgXm",
    "outputId": "800f1dc1-19a4-4234-fc0e-c9ffda45a16d"
   },
   "outputs": [],
   "source": [
    "df1_test.groupby(['Label']).size().plot.bar()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZFhdRC8AgXp",
    "outputId": "f986ad54-83c9-46b5-c999-9d334503c560"
   },
   "outputs": [],
   "source": [
    "df1_test.groupby(['Label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GyzNkI7W03D"
   },
   "outputs": [],
   "source": [
    "original_train_sentences =df1_train['Data'].values.tolist()\n",
    "original_labels_train =df1_train['Label'].values.tolist()\n",
    "original_test_sentences =df1_test['Data'].values.tolist()\n",
    "original_labels_test =df1_test['Label'].values.tolist()\n",
    "\n",
    "# example\n",
    "# original_train_sentences = ['this is sample 1','this is sample 2']\n",
    "# original_labels_train = ['postive','negative']\n",
    "# original_test_sentences = ['this is sample 1','this is sample 2']\n",
    "# original_labels_test = ['postive','negative']\n",
    "\n",
    "train_size = len(original_train_sentences)\n",
    "test_size = len(original_test_sentences)\n",
    "sentences = original_train_sentences + original_test_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K9dWTv5I07_"
   },
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TylTDwEnWD-l"
   },
   "outputs": [],
   "source": [
    "EDGE = 1 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
    "NODE = 0 # 0:one-hot #1:BERT\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "HIDDEN_DIM = 200\n",
    "DROP_OUT = 0.5\n",
    "#LR = 0.02\n",
    "LR =0.002\n",
    "WEIGHT_DECAY = 0\n",
    "EARLY_STOPPING = 10\n",
    "NUM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2W7wKTBfa71"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hobYcJ5OX5oT"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtWyhXiueMOq",
    "outputId": "96bb4236-9a67-441a-bd82-136e84be0c64"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "unique_labels=np.unique(original_labels_train)\n",
    "\n",
    "num_class = len(unique_labels)\n",
    "lEnc = LabelEncoder()\n",
    "lEnc.fit(unique_labels)\n",
    "\n",
    "print(unique_labels)\n",
    "print(lEnc.transform(unique_labels))\n",
    "\n",
    "train_labels = lEnc.transform(original_labels_train)\n",
    "test_labels = lEnc.transform(original_labels_test)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "labels = train_labels.tolist()+test_labels.tolist()\n",
    "labels = torch.LongTensor(labels).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMkEBxr6fMQi"
   },
   "source": [
    "## Remove Stopwords and less frequent words, tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xRG94uDfaBV",
    "outputId": "c89bde6b-2e1a-4e13-b43e-5356ee790971"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#url=\"https://docs.google.com/spreadsheets/d/14INZ1rR6R_l0xDFj_vCfCxGuM3IJ9YQZ/edit?usp=sharing&ouid=107542889467500911241&rtpof=true&sd=true\"\n",
    "#datapath='https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "#data1 =pd.read_excel(datapath)\n",
    "#stop_words = data1['words'].tolist()\n",
    "remove_limit = 5\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "original_word_freq = {}  # to remove rare words\n",
    "for sentence in sentences:\n",
    "    temp = clean_str(sentence)\n",
    "    word_list = temp.split()\n",
    "    for word in word_list:\n",
    "        if word in original_word_freq:\n",
    "            original_word_freq[word] += 1\n",
    "        else:\n",
    "            original_word_freq[word] = 1\n",
    "\n",
    "tokenize_sentences = []\n",
    "word_list_dict = {}\n",
    "for sentence in sentences:\n",
    "    temp = clean_str(sentence)\n",
    "    word_list_temp = temp.split()\n",
    "    doc_words = []\n",
    "    for word in word_list_temp:\n",
    "        if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
    "            doc_words.append(word)\n",
    "            word_list_dict[word] = 1\n",
    "    tokenize_sentences.append(doc_words)\n",
    "word_list = list(word_list_dict.keys())\n",
    "vocab_length = len(word_list)\n",
    "\n",
    "#word to id dict\n",
    "word_id_map = {}\n",
    "for i in range(vocab_length):\n",
    "    word_id_map[word_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqLUncB2Pn_L"
   },
   "outputs": [],
   "source": [
    "node_size = train_size + vocab_length + test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0o8wcXgrTiD"
   },
   "source": [
    "# Model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZbRV2wYxY1U"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znJ7Grz7fQ2L"
   },
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BSg1uNgV3_7"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "row = []\n",
    "col = []\n",
    "weight = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QESQPT88AqsI"
   },
   "source": [
    "### word-word: PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNlJoLFagXhv",
    "outputId": "6a3f97e5-6755-4604-e0a8-065ed7ac7ec0"
   },
   "outputs": [],
   "source": [
    "if EDGE >= 1:\n",
    "    window_size = 20\n",
    "    total_W = 0\n",
    "    word_occurrence = {}\n",
    "    word_pair_occurrence = {}\n",
    "\n",
    "    def ordered_word_pair(a, b):\n",
    "        if a > b:\n",
    "            return b, a\n",
    "        else:\n",
    "            return a, b\n",
    "\n",
    "    def update_word_and_word_pair_occurrence(q):\n",
    "        unique_q = list(set(q))\n",
    "        for i in unique_q:\n",
    "            try:\n",
    "                word_occurrence[i] += 1\n",
    "            except:\n",
    "                word_occurrence[i] = 1\n",
    "        for i in range(len(unique_q)):\n",
    "            for j in range(i+1, len(unique_q)):\n",
    "                word1 = unique_q[i]\n",
    "                word2 = unique_q[j]\n",
    "                word1, word2 = ordered_word_pair(word1, word2)\n",
    "                try:\n",
    "                    word_pair_occurrence[(word1, word2)] += 1\n",
    "                except:\n",
    "                    word_pair_occurrence[(word1, word2)] = 1\n",
    "\n",
    "\n",
    "    for ind in tqdm(range(train_size+test_size)):\n",
    "        words = tokenize_sentences[ind]\n",
    "\n",
    "        q = []\n",
    "        # push the first (window_size) words into a queue\n",
    "        for i in range(min(window_size, len(words))):\n",
    "            q += [word_id_map[words[i]]]\n",
    "        # update the total number of the sliding windows\n",
    "        total_W += 1\n",
    "        # update the number of sliding windows that contain each word and word pair\n",
    "        update_word_and_word_pair_occurrence(q)\n",
    "\n",
    "        now_next_word_index = window_size\n",
    "        # pop the first word out and let the next word in, keep doing this until the end of the document\n",
    "        while now_next_word_index<len(words):\n",
    "            q.pop(0)\n",
    "            q += [word_id_map[words[now_next_word_index]]]\n",
    "            now_next_word_index += 1\n",
    "            # update the total number of the sliding windows\n",
    "            total_W += 1\n",
    "            # update the number of sliding windows that contain each word and word pair\n",
    "            update_word_and_word_pair_occurrence(q)\n",
    "\n",
    "    for word_pair in word_pair_occurrence:\n",
    "        i = word_pair[0]\n",
    "        j = word_pair[1]\n",
    "        count = word_pair_occurrence[word_pair]\n",
    "        word_freq_i = word_occurrence[i]\n",
    "        word_freq_j = word_occurrence[j]\n",
    "        pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
    "        if pmi <=0:\n",
    "            continue\n",
    "        row.append(train_size + i)\n",
    "        col.append(train_size + j)\n",
    "        weight.append(pmi)\n",
    "        row.append(train_size + j)\n",
    "        col.append(train_size + i)\n",
    "        weight.append(pmi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hynLnT3a33kW"
   },
   "source": [
    "### doc-word: Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnSPqhg1lHps"
   },
   "outputs": [],
   "source": [
    "#get each word appears in which document\n",
    "word_doc_list = {}\n",
    "for word in word_list:\n",
    "    word_doc_list[word]=[]\n",
    "\n",
    "for i in range(len(tokenize_sentences)):\n",
    "    doc_words = tokenize_sentences[i]\n",
    "    unique_words = set(doc_words)\n",
    "    for word in unique_words:\n",
    "        exsit_list = word_doc_list[word]\n",
    "        exsit_list.append(i)\n",
    "        word_doc_list[word] = exsit_list\n",
    "\n",
    "#document frequency\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "# term frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(tokenize_sentences)):\n",
    "    words = tokenize_sentences[doc_id]\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6elPPFO_sXp"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokenize_sentences)):\n",
    "    words = tokenize_sentences[i]\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_length)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAr6ygKhWTc-"
   },
   "source": [
    "### doc-doc: jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4-EH15oWWSX"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "if EDGE>=2:\n",
    "    tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
    "    jaccard_threshold = 0.2\n",
    "    for i in tqdm(range(len(tokenize_sentences))):\n",
    "        for j in range(i+1, len(tokenize_sentences)):\n",
    "            jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
    "            if jaccard_w > jaccard_threshold:\n",
    "                if i < train_size:\n",
    "                    row.append(i)\n",
    "                else:\n",
    "                    row.append(i + vocab_length)\n",
    "                if j < train_size:\n",
    "                    col.append(j)\n",
    "                else:\n",
    "                    col.append(vocab_length + j)\n",
    "                weight.append(jaccard_w)\n",
    "                if j < train_size:\n",
    "                    row.append(j)\n",
    "                else:\n",
    "                    row.append(j + vocab_length)\n",
    "                if i < train_size:\n",
    "                    col.append(i)\n",
    "                else:\n",
    "                    col.append(vocab_length + i)\n",
    "                weight.append(jaccard_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIkGgB2aZDk7"
   },
   "source": [
    "### Adjacent matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0O1Ucdhod9a"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivyuexATkQFW"
   },
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
    "\n",
    "adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
    "\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMgbhTstMSUA"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-sqdNa6RC09",
    "outputId": "9b740a9e-b472-4b96-be6f-078392f76707"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('csebuetnlp/banglabert')\n",
    "example_text='আমি বাংলায় গান গাই'\n",
    "\n",
    "bert_input=tokenizer(example_text,padding='max_length',max_length=10,truncation=True,return_tensors=\"pt\")\n",
    "print(bert_input['input_ids'])\n",
    "print(bert_input['token_type_ids'])\n",
    "print(bert_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9p6k4E2ShD3",
    "outputId": "7eb5c9b5-d842-458f-b480-bc39378d59be"
   },
   "outputs": [],
   "source": [
    "example_text=tokenizer.decode(bert_input.input_ids[0])\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc-ESvvdSnf2",
    "outputId": "31e2b34b-0121-4ee3-c23f-b9d106cd46aa"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "from tqdm import tqdm\n",
    "lengths = []\n",
    "tk0 = tqdm(df1_train['Data'].fillna(\"\").values, total=len(df1_train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "max_len = max(lengths) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nM7TK6MfW2ds",
    "outputId": "23afcc00-7e16-44ef-bd1c-ee51909c13d0"
   },
   "outputs": [],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mP9dqCskOrXT"
   },
   "outputs": [],
   "source": [
    "\n",
    "if NODE == 0:\n",
    "    features = np.arange(node_size)\n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "else:\n",
    "\n",
    "    from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings\n",
    "    from flair.data import Sentence\n",
    "    doc_embedding = TransformerDocumentEmbeddings('csebuetnlp/banglabert', fine_tune=True)\n",
    "    word_embedding = TransformerWordEmbeddings('csebuetnlp/banglabert', layers='-1',subtoken_pooling=\"mean\")\n",
    "\n",
    "    sent_embs = []\n",
    "    word_embs = {}\n",
    "\n",
    "    for ind in tqdm(range(train_size+test_size)):\n",
    "        sent = tokenize_sentences[ind]\n",
    "        sentence = Sentence(\" \".join(sent[:max_len]),use_tokenizer=False)\n",
    "        doc_embedding.embed(sentence)\n",
    "        sent_embs.append(sentence.get_embedding().tolist())\n",
    "        words = Sentence(\" \".join(sent[:max_len]),use_tokenizer=False)\n",
    "        word_embedding.embed(words)\n",
    "        for token in words:\n",
    "            word = token.text\n",
    "            embedding = token.embedding.tolist()\n",
    "            if word not in word_embs:\n",
    "                word_embs[word] = embedding\n",
    "            else:\n",
    "                word_embs[word] = np.minimum(word_embs[word], embedding)\n",
    "\n",
    "    word_embs_list = []\n",
    "    for word in word_list:\n",
    "        word_embs_list.append(word_embs[word])\n",
    "\n",
    "    features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n",
    "\n",
    "    import scipy.sparse as sp\n",
    "    def preprocess_features(features):\n",
    "        \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "        rowsum = np.array(features.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = sp.diags(r_inv)\n",
    "        features = r_mat_inv.dot(features)\n",
    "        return features\n",
    "\n",
    "    features = preprocess_features(sp.csr_matrix(features)).todense()\n",
    "    features = torch.FloatTensor(features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFe3BlVnO7QU",
    "outputId": "c359b61f-b0c2-4380-a936-27304296e5a1"
   },
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdx6RrUvjbF0"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Kj8NQujiDH"
   },
   "source": [
    "## GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNVkA-h7b3sP"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.zeros(1, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters(in_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(drop_out)\n",
    "        self.activation =  activation\n",
    "\n",
    "    def reset_parameters(self,in_features, out_features):\n",
    "        stdv = np.sqrt(6.0/(in_features+out_features))\n",
    "        # stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        # if self.bias is not None:\n",
    "        #     torch.nn.init.zeros_(self.bias)\n",
    "            # self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input, adj, feature_less = False):\n",
    "        if feature_less:\n",
    "            support = self.weight\n",
    "            support = self.dropout(support)\n",
    "        else:\n",
    "            input = self.dropout(input)\n",
    "            support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k57M4sz4s4Md"
   },
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJ-ZQuMzs5tZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n",
    "        super(GCN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.gc_list = []\n",
    "        if n_layers >= 2:\n",
    "            self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n",
    "            self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n",
    "            self.gcf = GraphConvolution(nhid, nclass, dropout)\n",
    "        else:\n",
    "            self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        if self.n_layers>=2:\n",
    "            x = self.gc1(x, adj, feature_less = True)\n",
    "            for i in range(self.n_layers-2):\n",
    "                x = self.gc_list[i](x,adj)\n",
    "            x = self.gcf(x,adj)\n",
    "        else:\n",
    "            x = self.gc1(x, adj, feature_less = True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmhOG1yG--Ji"
   },
   "outputs": [],
   "source": [
    "def cal_accuracy(predictions,labels):\n",
    "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
    "    lab = labels.cpu().tolist()\n",
    "    cor = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == lab[i]:\n",
    "            cor += 1\n",
    "    return cor/len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEE4JxeUthCb"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIxII4QoticA"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdNsgxMG-Wwu"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T98r4qZuuFyn"
   },
   "source": [
    "## Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv9br9pgGw9R"
   },
   "outputs": [],
   "source": [
    "def generate_train_val(train_pro=0.9):\n",
    "    real_train_size = int(train_pro*train_size)\n",
    "    val_size = train_size-real_train_size\n",
    "\n",
    "    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
    "    idx_train.sort()\n",
    "    idx_val = []\n",
    "    pointer = 0\n",
    "    for v in range(train_size):\n",
    "        if pointer<len(idx_train) and idx_train[pointer] == v:\n",
    "            pointer +=1\n",
    "        else:\n",
    "            idx_val.append(v)\n",
    "    idx_test = range(train_size+vocab_length, node_size)\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "idx_train, idx_val, idx_test = generate_train_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QC7u3Jn2uIu4",
    "outputId": "1c4ad4ce-00bc-4ff3-fb81-71f324d42871"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(show_result = True):\n",
    "    val_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output= model(features, adj)\n",
    "        loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "        acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "        val_loss.append(loss_val.item())\n",
    "        acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n",
    "        if show_result:\n",
    "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
    "                    'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                    'acc_train: {:.4f}'.format(acc_train),\n",
    "                    'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                    'acc_val: {:.4f}'.format(acc_val),\n",
    "                    'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
    "            if show_result:\n",
    "                print(\"Early Stopping...\")\n",
    "            break\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQwlWq6dyYJm"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmPNukmk40gd",
    "outputId": "8ba1cb33-8ec0-4e94-df05-24f31fd1a5ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    predictions = torch.argmax(output[idx_test], -1).cpu().tolist()\n",
    "    test_labels1=t = torch.from_numpy(test_labels)\n",
    "    test_labels1 = test_labels1.to(device)\n",
    "\n",
    "    f13=criterion(output[idx_test],test_labels1).item()\n",
    "\n",
    "    acc = accuracy_score(test_labels, predictions)\n",
    "    f11 = f1_score(test_labels, predictions, average='macro')\n",
    "    f12 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "    # Compute probabilities from the model's output\n",
    "    # probabilities = torch.softmax(output[idx_test], dim=1).cpu().detach().numpy()\n",
    "\n",
    "    # Compute ROC-AUC score for each class\n",
    "    # f13 = roc_auc_score(np.eye(len(np.unique(test_labels)))[test_labels], probabilities, average='macro')\n",
    "\n",
    "    return acc, f11, f12, f13\n",
    "\n",
    "print(test())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOFsVlv4hTgc"
   },
   "source": [
    "# Test 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydMqrCkehVPW",
    "outputId": "5a6ab9c0-fa75-46ef-88cf-e11d22c8dee3"
   },
   "outputs": [],
   "source": [
    "test_acc_list = []\n",
    "test_f11_list = []\n",
    "test_f12_list = []\n",
    "test_f13_list = []\n",
    "\n",
    "for t in range(10):\n",
    "    model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    idx_train, idx_val, idx_test = generate_train_val()\n",
    "    train_model(show_result=False)\n",
    "    acc, f11, f12, f13 = test()\n",
    "    test_acc_list.append(acc)\n",
    "    test_f11_list.append(f11)\n",
    "    test_f12_list.append(f12)\n",
    "    test_f13_list.append(f13)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",np.round(np.mean(test_acc_list),4))\n",
    "print(\"Macro F1:\",np.round(np.mean(test_f11_list),4))\n",
    "print(\"Weighted F1:\",np.round(np.mean(test_f12_list),4))\n",
    "# print(\"ROC-AUC score:\",np.round(np.mean(test_f13_list),4))\n",
    "print(\"Accuracy:\",test_acc_list)\n",
    "print(\"Macro F1:\",test_f11_list)\n",
    "print(\"Weighted F1:\",test_f12_list)\n",
    "print(\"Loss:\",test_f13_list)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
